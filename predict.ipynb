{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "data_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(data, cabin=False):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    #function for separating initials\n",
    "#     def sepInitials(name):\n",
    "#         return name.split(',')[1].split('.')[0].strip()\n",
    "#     df_initials = pd.DataFrame({'Salutation':data['Name'].apply(sepInitials)})\n",
    "#     df_initials = pd.DataFrame({'Salutation':data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())})\n",
    "    data = pd.merge(data, pd.DataFrame({'Salutation':data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())}), left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    def group_salutation(sa):\n",
    "        if sa == 'Mr':\n",
    "            return 'Mr'\n",
    "        elif sa== 'Mrs':\n",
    "            return 'Mrs'\n",
    "        elif sa== 'Miss':\n",
    "            return 'Miss'\n",
    "        elif sa=='Master':\n",
    "            return 'Master'\n",
    "        else:\n",
    "            return 'others'\n",
    "    df_3 = pd.DataFrame({'g_salutation': data['Salutation'].apply(group_salutation)})\n",
    "    data = pd.merge(data, df_3, left_index=True, right_index=True)\n",
    "\n",
    "    table = data.pivot_table(values='Age', index=['g_salutation'], columns=['Pclass', 'Sex'], aggfunc=np.median)\n",
    "    \n",
    "    def fage(x):\n",
    "        return table[x['Pclass']][x['Sex']][x['g_salutation']]\n",
    "    data['Age'].fillna(data[data['Age'].isnull()].apply(fage, axis=1), inplace=True)\n",
    "\n",
    "    data.drop('Name', axis=1, inplace=True)\n",
    "    title_dumies = pd.get_dummies(data['g_salutation'], prefix='g_salutation')\n",
    "    data = pd.concat([data, title_dumies], axis=1)\n",
    "    data.drop('g_salutation', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    data.Embarked.fillna(data.Embarked.max(), inplace=True)\n",
    "    embarked_dummies = pd.get_dummies(data['Embarked'], prefix='Embarked')\n",
    "    data = pd.concat([data, embarked_dummies], axis=1)\n",
    "    data.drop('Embarked', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    #processing gender\n",
    "    data['Sex'] = data['Sex'].map({'male':1, 'female':0})\n",
    "\n",
    "\n",
    "\n",
    "    #encoding and cleaning cabin\n",
    "    data.Cabin.fillna('U', inplace=True)\n",
    "    #mapping each cabin value with the cabin letter\n",
    "    data['Cabin'] = data['Cabin'].map(lambda c:c[0])\n",
    "    cabin_dummies = pd.get_dummies(data['Cabin'], prefix='Cabin')\n",
    "    data = pd.concat([data, cabin_dummies], axis=1)\n",
    "    data.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    pclass_dummies = pd.get_dummies(data['Pclass'], prefix='Pclass')\n",
    "    data = pd.concat([data, pclass_dummies], axis=1)\n",
    "    data.drop('Pclass', axis=1, inplace=True)\n",
    "\n",
    "    #creating a new feature called family size\n",
    "    data['FamilySize'] = data['Parch'] + data['SibSp'] +1 \n",
    "    data['Singleton'] = data['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "    data['SmallFamily'] = data['FamilySize'].map(lambda s: 1 if 2<=s<=4 else 0)\n",
    "    data['LargeFamily'] = data['FamilySize'].map(lambda s: 1 if 5<=s else 0)\n",
    "\n",
    "    data.drop(['Salutation','Ticket', 'PassengerId'], axis=1, inplace=True)\n",
    "    \n",
    "    if cabin:\n",
    "        data.drop(['Cabin_T'], axis=1, inplace=True)\n",
    "\n",
    "    if (data.Age.isnull().sum() > 0):\n",
    "        data_test.Age.fillna(data_test.Age.median(), inplace=True)\n",
    "    if (data.Fare.isnull().sum() > 0):\n",
    "        data_test.Fare.fillna(data_test.Fare.median(), inplace=True)\n",
    "\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepareData(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,1:]\n",
    "targets = data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(clf, X, y, scoring='accuracy'):\n",
    "    xval = cross_val_score(clf, X, y, cv = 2, scoring=scoring)\n",
    "    return np.mean(xval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, max_features='sqrt')\n",
    "clf = clf.fit(x, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f87ce9a14d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.DataFrame()\n",
    "features['feature'] = x.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "features.set_index('feature', inplace=True)\n",
    "\n",
    "features.plot(kind='barh', figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 17)\n"
     ]
    }
   ],
   "source": [
    "# #transforming the model, removing the poor performance features\n",
    "model = SelectFromModel(clf, prefit=True, threshold=0.012120)\n",
    "train_reduced = model.transform(x)\n",
    "print train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_reduced, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #grid search\n",
    "# pipeline = Pipeline([\n",
    "#     ('clf', RandomForestClassifier(criterion='entropy'))\n",
    "# ])\n",
    "# parameters = {\n",
    "#     'clf__n_estimators': (5, 10, 20, 50),\n",
    "#     'clf__max_depth': (50, 150, 250),\n",
    "#     'clf__min_samples_split': (1, 2, 3),\n",
    "#     'clf__min_samples_leaf': (1, 2, 3)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1')\n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n",
    "# print 'Best score: %0.3f' % grid_search.best_score_\n",
    "# print 'Best parameters set:'\n",
    "\n",
    "# best_parameters = grid_search.best_estimator_.get_params()\n",
    "# for param_name in sorted(parameters.keys()):\n",
    "#     print '\\t%s: %r' % (param_name, best_parameters[param_name])\n",
    "    \n",
    "# predictions = grid_search.predict(x_test)\n",
    "# print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=3,\n",
       "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'bootstrap': False, 'min_samples_leaf': 3, 'n_estimators': 50, \n",
    "                  'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 6}\n",
    "    \n",
    "model = RandomForestClassifier(**parameters)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82511210762331844"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
